# 30 Days of AI Voice Agents Challenge by Murf AI ğŸš€

This repository documents my progress in the **#30DaysofVoiceAgents** challenge by **Murf AI**.  
Each day focuses on building a fully functional AI-powered voice application, step-by-step, using **FastAPI**, **JavaScript**, **HTML**, **Murf API**, **AssemblyAI**, and **Google Gemini API**.


## Project Structure
```
AI voice agents/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_service.py
â”‚   â”‚   â”œâ”€â”€ stt_service.py
â”‚   â”‚   â””â”€â”€ tts_service.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ session_manager.py
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ script.js
â”‚   â””â”€â”€ styles.css
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ uploads/
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ app.log
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
```

---

## ğŸ“… Daily Progress

### **Day 1: Project Setup**
- Initialized a **Python FastAPI** backend.
- Created a basic `index.html` and `script.js` for the frontend.
- Served the HTML page from the Python server.

**Skills Learned:**  
âœ… FastAPI setup  
âœ… Static file serving in FastAPI  

---

### **Day 2: Your First REST TTS Call**
- Created `/generate-audio` endpoint.
- Integrated **Murf REST TTS API** to convert input text to speech.
- Returned an **audio URL** in the API response.
- Secured API keys using `.env`.

**Skills Learned:**  
âœ… REST API integration  
âœ… Environment variable security  

---

### **Day 3: Playing Back TTS Audio**
- Added a **text input** and **submit button** in the frontend.
- Sent the text to `/generate-audio` endpoint.
- Played the returned audio using `<audio>` tag in HTML.

**Skills Learned:**  
âœ… Fetch API in JavaScript  
âœ… HTML audio playback  

---

### **Day 4: Echo Bot v1**
- Added **"Echo Bot"** section in UI.
- Used the **MediaRecorder API** to record audio from the microphone.
- Created **Start** and **Stop** buttons to control recording.
- Played back recorded audio in `<audio>` element.

**Skills Learned:**  
âœ… MediaRecorder API  
âœ… Handling audio blobs in JS  

---

### **Day 5: Sending Audio to the Server**
- Modified Echo Bot to **upload recorded audio** to the server after stopping recording.
- Built `/upload-audio` endpoint in FastAPI.
- Saved uploaded audio in `/uploads` folder.
- Returned **file name, content type, and size** in response.

**Skills Learned:**  
âœ… File uploads in FastAPI  
âœ… Handling `UploadFile` in Python  

---

### **Day 6: Server-Side Transcription**
- Created `/transcribe/file` endpoint.
- Integrated **AssemblyAI** to transcribe uploaded audio.
- Displayed transcription in the UI.

**Skills Learned:**  
âœ… AssemblyAI transcription API  
âœ… Working with binary audio data in Python  

---

### **Day 7: Echo Bot v2**
- Created `/tts/echo` endpoint.
- Transcribed uploaded audio using AssemblyAI.
- Sent transcription to Murf API to generate speech in a Murf voice.
- Played Murf-generated audio in the Echo Bot UI.

**Skills Learned:**  
âœ… Combining APIs for multi-step workflows  
âœ… Real-time audio transformation  

---

### **Day 8: Integrating a Large Language Model (LLM)**
- Created `/llm/query` endpoint.
- Integrated **Google Gemini API** to generate responses to user input.
- Returned LLM output as API response.

**Skills Learned:**  
âœ… LLM API integration  
âœ… Building conversational endpoints  

---

### **Day 9: The Full Non-Streaming Pipeline**
- Updated `/llm/query` endpoint.
- Accept audio as input.
- Audio is transcribed through **AssemblyAI**
- Transciption sent to **Google Gemini API**
- **Gemini API** text response is converted into Murf Voice using **Murf API**

**Skils Learned:**
âœ… Combining LLM API with other for multi-step workflows  
âœ… Real-time audio response from LLM

---

### **Day 10: Chat History for Conversational Bot**
- Created a POST `/agent/chat/{session_id}` endpoint.
- Used an in-memory dictionary datastore to store chat history per session_id.
- Chat history for that session_id is fetched and combined with the new message and sent to LLM
- LLM response is stored back into chat history and audio response is given to user.
-Automatically starts recording after an LLM audio response

**Skills Learned:**
âœ… Session-based conversation memory
âœ… Combining STT â†’ LLM â†’ TTS in one flow
âœ… Auto-triggering recording in the frontend

---

### **Day 11: Building Resilient AI Voice Applications: Mastering Error Handling ğŸš¨**
ğŸ”¥ Scenarios I tackled:
- API & Service Failures
- Audio Processing Issues
- Network & Connectivity Problems
- Session Stability

**Skills Learned:**
âœ… More efficient error handling

---

### **Day 12: Enhanced UI and also removed echo bot & AI voice generator**
- â• â€œNew Sessionâ€ button 
- Changed the old looking UI to modern LLM UI's like Gemini, ChatGPT,etc 
- Single toggle start/stop recording button
- âŒ Stop Conversation button at bottom right â€” stops auto-mic loop after agent responses
- Transcribed conversation text displayed above buttons, along with LLM status updates
- Removed initial TTS & Echo Bot sections for a cleaner layout

**Skills Learned:**
âœ… Building more convenient UI
âœ… Making UI like Modern LLMs

---

### **Day 13: Create a Readme file if not created yet**
- Already Done!

---

### **Day 14: Code Refactoring & Project Cleanup ğŸ› ï¸**
- Separated **schemas** (Pydantic models) for request/response objects into `/models` for cleaner endpoint definitions.
- Moved **STT**, **TTS**, and **LLM** integrations into dedicated files under `/services`.
- Added `session_manager.py` in `/utils` for managing chat sessions and history.
- Added `config.py` in `/main` for centralized configuration and environment variable management.
- Created `__init__.py` in every folder for proper Python package structure.
- Removed unused imports, variables, and functions.
- Improved code readability with clear function names and docstrings.
- Updated README with latest project structure.

**Skills Learned:**
âœ… Code refactoring best practices  
âœ… Organizing a FastAPI project for maintainability  
âœ… Using Pythonâ€™s `logging` module effectively  
âœ… Writing cleaner, more modular code

---
### **Streaming Branch** 
---

### **Day 15: WebSocket Connection**
- Created a **`/ws` WebSocket endpoint** in FastAPI.  
- Established real-time connection between **client â†” server**.  
- Implemented echo functionality: server sends back the same message received from client (tested via Postman).  
- Worked on a **separate `streaming` branch** to keep non-streaming code safe.  

**Skills Learned:**  
âœ… FastAPI WebSocket setup  
âœ… Real-time message echo testing  

---

### **Day 16: Streaming Audio**
- Extended recording logic on client to **stream audio chunks via WebSockets** at intervals.  
- Server received binary audio data through WebSocket and **saved it to a file**.  
- Verified audio reception and storage (no transcription/LLM yet).  

**Skills Learned:**  
âœ… Streaming binary data over WebSockets  
âœ… Audio file handling on the server  

---

### **Day 17: WebSockets + AssemblyAI**
- Integrated **AssemblyAI Python SDK** with streaming WebSocket audio.  
- Converted audio into required **16kHz, 16-bit, mono PCM format**.  
- Transcriptions were generated in real-time and printed to console (or UI).  

**Skills Learned:**  
âœ… Live transcription with AssemblyAI  
âœ… Audio preprocessing for STT  

---

### **Day 18: Turn Detection**
- Used AssemblyAIâ€™s **streaming API with turn detection**.  
- Detected when the user stopped talking.  
- Server notified client of **end-of-turn events** via WebSocket.  
- Displayed **finalized transcription** on the UI after each turn.  

**Skills Learned:**  
âœ… Turn detection in speech streams  
âœ… Real-time server â†’ client updates via WebSocket  

---

### **Day 19: Streaming LLM Responses**
- After receiving a finalized transcript â†’ sent it to **Gemini LLM API**.  
- Used `streamGenerateContent` to **stream LLM response chunks**.  
- Accumulated responses and printed them to console (no UI yet).  

**Skills Learned:**  
âœ… Streaming responses from LLM  
âœ… Handling incremental response chunks 

---

### **Day 20: Streaming LLM â†’ Murf Voice via WebSockets**
- Integrated **Gemini LLM streaming responses** with **Murf TTS** over WebSockets.  
- Workflow:  
  1. Sent **streaming LLM responses** (from Gemini) to Murf WebSocket API.  
  2. Murf returned **audio in base64 format**.  
  3. Printed encoded audio directly in server console for verification.  
  4. Used a static **`context_id`** in Murf WebSocket requests to avoid context overflow.  

**Skills Learned:**  
âœ… Streaming text â†’ speech in real-time  
âœ… Handling base64 audio over WebSockets  
âœ… Managing Murf WebSocket sessions with context_id  

---

### **Day 21: Streaming Audio Data to the Client**
- Extended WebSocket logic to **send streaming audio data from server â†’ client** in real time.  
- On the client side:  
  - **Accumulated base64 chunks** into an array.  
  - Printed **acknowledgements** on console confirming successful reception of audio data.  

**Skills Learned:**  
âœ… Server â†’ client streaming over WebSockets  
âœ… Handling real-time base64 audio data  
âœ… Ensuring reliable audio delivery with acknowledgements  

---

## ğŸ› ï¸ Tech Stack

### **Backend**
- **FastAPI (Python)** â€“ Core backend framework
- **WebSockets (FastAPI + WebSocket API)** â€“ Real-time audio & text streaming
- **uvicorn** â€“ ASGI server for running FastAPI apps

### **Frontend**
- **HTML, CSS, JavaScript**
- **WebSocket API (Browser)** â€“ Sending/receiving streaming data

### **AI & APIs**
- **Murf API** â€“ Text-to-Speech (REST + WebSocket)
- **AssemblyAI API** â€“ Speech-to-Text (file upload + streaming + turn detection)
- **Google Gemini API** â€“ Large Language Model (text + streaming responses)

### **Data Handling**
- **In-memory datastore (Python dict)** â€“ Session-based chat history
- **Base64 encoding** â€“ Audio transmission
- **PCM Audio (16kHz, 16-bit, mono)** â€“ Format required for STT streaming
- **File uploads & storage** â€“ Temporary audio storage for testing

### **Other Tools**
- **dotenv** â€“ API key management via `.env`
- **Postman** â€“ API testing
- **Git + GitHub** â€“ Version control
- **Branching strategy** â€“ Separate `streaming` branch for WebSocket features


---
